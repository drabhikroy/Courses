---
title: "Missingness"
linktitle: "Week 7: Missingness "
toc: true
output:
  rmarkdown::html_document:
    toc: true
    self_contained: false
menu:
  trainings:
    parent: Walkthroughs
    weight: 7
type: docs
weight: 2
---

`r blogdown::shortcode("r-walkthrough-header")`

```{r libraries, message=FALSE, warning=FALSE, include=FALSE}
source(here::here("content", "com_libs.R"))
```

```{r setup, include=FALSE, purl=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(widgetframe_self_contained = TRUE) 
knitr::opts_knit$set(root.dir = getwd())

library(tidyverse)
library(naniar)
library(mice)
library(reactable)
library(knitr)
library(kableExtra)
library(htmlwidgets)
library(htmltools)
```

# Multiple Imputation

```{r echo = FALSE, purl=FALSE,}

dplyr::tibble(
  
  information <- c(
    "<b>Level of Difficulty</b><sup>1</sup>",
    "<b>What You'll Mainly Do</b>",
    "<b>Language(s) We'll Use</b><sup>2</sup>",
    ""
  ),
  
  description <- c(
    "<span style='color:#c5d9ff'>Advanced</span>",
    "Load an external csv file, type in syntax, and consider interpretations",
    "<div id='info'><div id='speechbubble' data-hover='R'><span style='color:#4682b4'><i class='fa-brands fa-r-project fa-xl'></i></span></div></div>",
    ""
            ),
  
  .name_repair = "minimal"
  
) %>%
  kbl(col.names = c("", ""), 
      "html", 
      escape = FALSE,
      align = 'll') %>%
  kable_paper() %>%
  kable_styling(full_width = FALSE,
                font_size = 20,
                position = "center") %>%
  column_spec(1, width = "12em") %>%
  column_spec(2, width = "30em") %>%
  row_spec(0:3, extra_css = 'vertical-align: middle !important;', background = "transparent", color = "#f7f7f7") %>%
  row_spec(3, extra_css = "margin-bottom: 30px;") %>%
  add_footnote(
    c(
      "<span style='color:#f7f7f7; font-size:11pt;'><i>This is not an indicator of your personal level or abilities. You may experience varying level of ease and/or difficulties and that is OK!</i></span>", 
      "<span style='color:#f7f7f7; font-size:11pt;'><i>Hover over an icon to see its name</i></span>"), 
    notation = "number", 
    escape = FALSE
    )
```

## Prerequisites

+ Please go and take the [Nerdy Personality Attributes Scale](https://openpsychometrics.org/tests/NPAS/){target="_blank"}. 

+ Read the paper [An exploratory factor analysis of the Nerdy Personality Attributes Scale in a sample of self-identified nerds/geeks (Finister, Pollet, & Neave, 2020)](/readings/07-readings/#download-literature){target="_blank"} included in this week's downloads. . 

## Materials

The following download contains files that are provided to help you to understand and/or complete the walkthrough

<center>
```{r echo=FALSE,eval=TRUE,message=FALSE,purl=FALSE}
loc <- here::here("static", "data", "EDP619Week7Walkthrough.zip")

downloadthis::download_file(
  path = loc,
  output_name = "Week 7 Walkthrough",
  button_label = "&nbsp;&nbsp;&nbsp;<span style='color:#ffffff'>Download</span>",
  button_type = "primary",
  has_icon = TRUE,
  icon = "fa fa-save",
  class = "hvr-sweep-to-left"
  )
```
</center>

## Prepping

1. Go through the [slideshow](/tasks/factors) this week prior to engaging the walkthrough provided below unless you have a strong familiarity with factor validity. 

2. Like last week, open up Rstudio and create a new script by going to **File > New File > R Script**. Save this in an easily accessible folder. Now unzip this week's data set and take the files - `NerdyDataMissing.csv`, `NerdyCodebook_PM.csv`, `NerdyMeasures_PM.csv`, `Nerdy install.R`, and `Nerdy script.R` - and drop them all in a folder of their own. The remaining files have additional data you can practice with.

3. Open the file `Nerdy install.R` and run the syntax to install the packages you will need for this task. If you are asked to update any packages, please select `1: All`. 

4. Now please go ahead and load up the following libraries

<div class = "rounded">
```{r eval=FALSE}
library("tidyverse")
library("mice")
library("naniar")
library("reactable")
```
</div>

4. set the working directory to the location of the script by running

<div class = "rounded">
```{r eval=FALSE}
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
```
</div>

in your console or by selecting **Session > Set Working Directory > To Source File Location**.

## Sampling Frame

We'll be looking at responses from a sample of 11 people who took this test in 2018. 

### Loading a Local Data Set

If you remember how to load a local dataset, then please skip this section

<div class = "rounded">
```{r eval=FALSE}
nerdy_data <- 
  read_csv("NerdyDataMissing.csv")
```
</div>

```{r echo=FALSE, purl=FALSE}
nerdy_data <- 
  read_csv(here::here("static", "data", "NerdyDataMissing.csv"))
```

<div class = "rounded">
```{r eval=FALSE}
nerdy_pm_codebook <- 
  read_csv("NerdyCodebook_PM.csv")
```
</div>

```{r echo=FALSE, purl=FALSE}
nerdy_pm_codebook <-
  read_csv(here::here("static", "data", "NerdyCodebook_PM.csv"))
```

<div class = "rounded">
```{r eval=FALSE}
nerdy_pm_measures <- 
  read_csv("NerdyMeasures_PM.csv")
```
</div>

```{r echo=FALSE, purl=FALSE}
nerdy_pm_measures <-
  read_csv(here::here("static", "data", "NerdyMeasures_PM.csv"))
```

and take a look at the data

<div class = "rounded">
```{r}
nerdy_data %>%
  head()
```
</div>

or via the `reactable()` package^[Scroll to the right to see all columns]

```{r echo=FALSE}

reactable(nerdy_data,
          searchable = FALSE, 
          defaultPageSize = 5,
          showPageSizeOptions = TRUE,
          highlight = FALSE,
          defaultColDef = colDef(
            align = "center"
          ),
          theme = reactableTheme(
            borderColor = "#dfe2e5",
            backgroundColor = "#212121",
            stripedColor = "#f6f8fa",
            highlightColor = "#f0f5f9",
            cellPadding = "8px 12px",
            style = list(
              fontFamily = "Roboto Condensed"
              ),
            searchInputStyle = list(width = "20%")
            )
          )
```

<div class = "rounded">
```{r eval=FALSE}

reactable(nerdy_data,
          defaultPageSize = 5,
          showPageSizeOptions = TRUE)

```
</div>

as well as the codebook and measures

<div class = "rounded">
```{r}
nerdy_pm_codebook
```
</div>

<div class = "rounded">
```{r}
nerdy_pm_measures
```
</div>

## Missingness

You may be asking *why should I care about missing data?* and *can't I just delete them?* Great questions! 

Some analyses like linear regression that you covered this week require complete observations, but still in most statistical software you won’t get an error when feeding the system with data containing missing values (cough cough but R does tell you there's an error cough cough). 

Instead, most non R softwares automatically delete incomplete cases...and silently at that which is pretty rude. However, if you plan to say make conclusions about the entire data set. an interpretation just wouldn’t be appropriate since you cannot guarantee the same observations. Even worse, by dropping the observations completely we lose statistical power which in a nutshell results in a likelihood that you'll conclude that there isn't an effect when there is (aka a Type II error). Oh but it gets even better, the dropped observations could provide crucial information about the problem of interest resulting in biased results. 

In practice, this can be the difference between being able to *conclude a treatment likely works* and *reporting maybe something happened but we don't know*. So missing data is a big problem that we still haven't been able to tackle directly yet^[...though recent advances in machine learning have brought us closer in the past three years than we have had in the past 50]. 

To tackle this issue, there are two prevailing philosophical approaches. On one side are those who believe that the data is what it is and you have to deal with what you get. On the other side you have those that believe a machine, whether it be using an algorithm by itself or coupled with artificial intelligence, can predict the likely values that should have been there given certain assumptions are met (which we cover in the next section)^[In all fairness, I am in this group and absolutely biased]. 

So for the latter group, there are a number of tactics that can be used to tackle this missingness issue, but the main three are

1. **pairwise deletion**: a method in which data for a variable pertinent to a specific assessment are included, even if values for the same individual on other variables are missing.

2. **listwise deletion**: a method in which an entire case record is excluded from statistical analysis if values are found to be missing for any variable of interest.

3. **imputation**: a procedure for filling in missing values in a data set before analyzing the resultant completed data set.

Most people in this camp use the deletion techniques as a last resort because there is wariness about removing data. In this walkthrough, we'll cover the last approach: imputation.

### Note

Please be aware that this idea of addressing missingness in a data set is a multifaceted issue, there is a lot that is not covered here, and in practice it is very easy to go down a *rabbit hole*. This walkthrough is written in a way that hopefully brings a high level topic down to an understandable level. In doing that, some information is lost or has been set aside for the sake of simplicity. With that said, view this through the lens of a problem that you might or will even likely come across. The issue is that few people know what to do about missing data so they either ignore or delete it without considering the ramifications. However if you are interested in learning more or need clarifications, please reach out!

## Assumptions

<center>
<p id="rounded_corners_long">
While there are approaches to imputing discrete data, we'll be covering the case for those items that have scales considered to be continuous (e.g. Likert type) only!
</p>
</center>

If you have taken statistics in the past, then you likely know about these things called *assumptions*, or criteria that have to be satisfied before running a statistical test (e.g. *t*-test, ANOVA, etc.). This is also true when dealing with missing data points, in that we have to determine if our data is

```{r echo=FALSE,purl=FALSE}

type <- tibble(
  
  missingness <- c(
    "Missing Completely At Random",
    "Missing At Random",
    "Missing Not At Random"
  ),
  
  short <- c("MCAR",
             "MAR",
             "MNAR"
             ),

  description <- c(
    "The locations of missing values are random and not dependent on other data.",
    "The locations of missing values are random BUT depend on some other observed data",
    "There is a pattern to the missing values"
    ),

  highlights <- c(
    "Strong assumptions; difficult or possibly impossible to detect",
    "Neutral(ish) assumptions; possible to detect",
    "Weak assumptions; easy to detect"
    ),
    
  do <- c(
    "Deletion techniques or imputation with confidence in reporting",
    "Advanced imputation with caution in reporting.",
    "An inability to report anything meaningful (get new or more data because this is bad)."
    ),
  
  .name_repair = "minimal"
  
)

```

<center>
```{r, echo=FALSE,purl=FALSE}
type %>%
  kbl(col.names = c("Type", "Shorthand", "Description", "Traits", "What Can You Do?"), 
      "html", 
      escape = FALSE,
      align = 'lllll') %>%
  kable_paper(html_font = "Roboto Condensed",
              font_size = "13pt") %>% 
  kable_styling(position = "center",
                full_width = FALSE) %>%
  column_spec(1, width = "10em") %>%
  column_spec(2, width = "10em") %>%
  column_spec(3, width = "30em") %>%
  column_spec(4, width = "30em") %>%
  column_spec(5, width = "30em") %>%
  row_spec(0:3, extra_css = 'vertical-align: middle !important;', background = "transparent", color = "#f7f7f7") 
```
</center>

## What to Do About Missing Data

In the real world, data involving humans is rarely results in a complete set of values. As you can imagine, this is nearly always the case when conducting surveys where respondents are free to skip questions^[You're first reaction may be to require respondents to address items -aka a forced response- but that almost always backfires resulting in a higher rate of nonresponses]. So what can you do? 

### Testing for MCAR

While it is difficult, if not impossible to figure out if a dataset is MCAR, we can use probability to determine if it could be. The best way to do this is to figure out a formula for the regression, and to test that - but we're not going to take that approach given the makeup of the course. A simpler, yet less robust way is to use a technique developed by [Little (1988)](/handouts/Little%20(1988).pdf){target="_blank"} that basically gives is a probability that a data set is MCAR. But remember, there is always an associated probability that it may not be too!

Using the `narniar` package, we can use the test. In a callback to your introductory statistics course, the null hypothesis ($H_0$) is that the data is MCAR while the test statistic is a chi-squared ($\chi^2$) value.

<div class = "rounded">
```{r}
mcar_test(nerdy_data)
```
</div>

Given the high $p$-value, we can say that `nerdy_data` *could be* MCAR. To get an idea of the number of items that have missing values, you can use 

<div class = "rounded">
```{r}
n_var_miss(nerdy_data)
```
</div>

To view the missingness by item, we can use the following

<div class = "rounded">
```{r}
gg_miss_var(nerdy_data) +
  guides(color = "none")
```
</div>

which just says all 26 columns have at least one missing value. To see common missingness by items, we can use

<div class = "rounded">
```{r}
gg_miss_upset(nerdy_data,
              nsets = n_var_miss(nerdy_data))
```
</div>

where `nsets` are the number of columns which shows us

* all of the variables with missing values
* all of the common values that are missing
* each combination of missing values occurs once

but that is really difficult to read and we're just getting a feel for the missingness, so let's try limiting the  number of variables to say the top 5

<div class = "rounded">
```{r}
gg_miss_upset(nerdy_data,
              nsets = 5)
```
</div>

OK that's better! Now its easier to see questions 15, 22, and 23 share a common missing value and so forth.

We can even use `ggplot` to visualize this differently by item. First we have to add a column for the respondents by 

<div class = "rounded">
```{r}
nerdy_data_rowid <- 
  nerdy_data %>% 
  rowid_to_column("respondent")
```
</div>

which just adds a column in front with unique ids.

<div class = "rounded">
```{r}
nerdy_data_rowid
```
</div>

And then let's visualize it for Q1

<div class = "rounded">
```{r}
ggplot(data = nerdy_data_rowid, 
       aes(x = respondent,
       y = Q1)) +
  geom_miss_point()
```
</div>

or maybe its easier without the default grey background and bigger

<div class = "rounded">
```{r}
ggplot(data = nerdy_data_rowid, 
       aes(x = respondent,
       y = Q1)) +
  geom_miss_point(size = 2) +
  theme_minimal()
```
</div>

On a side note, yes the *x*-axis looks odd and we could fix it, but we're just exploring.

### Multiple Imputation

Run the command `mice()` from the aptly named `mice` package to impute a data set. Here `m` is the number of times your computer applies the imputation algorithm to the data set

<div class = "rounded">
```{r, message=FALSE, results='hide'}
nerdy_data_imputed <- 
  mice(nerdy_data, 
       m = 15, 
       method = 'pmm')  %>%
  complete(1) %>%
  as_tibble()
```
</div>

<div class = "rounded">
```{r}
nerdy_data_imputed
```
</div>

Now there are a lot of ways to impute data, some which will do a better job than others and dependent of what type of data you have. To see a list, just run `?mice::mice` and scroll about halfway down the help page. You can change the approach by replacing `pmm` in `method = 'pmm'` above. 

Anyway is our new data set any better because it look like there are still missing values. Well let's take a look

<div class = "rounded">
```{r}
n_var_miss(nerdy_data_imputed)
```
</div>

<div class = "rounded">
```{r}
gg_miss_var(nerdy_data_imputed) +
  guides(color = "none")
```
</div>

<div class = "rounded">
```{r}
gg_miss_upset(nerdy_data_imputed)
```
</div>

Oh that looks a lot better! We could change `m` or the `method` to get a more complete data set.

## The Data
I randomized missing data the original subset which can be seen by opening the `NerdyDataSubset.csv` file. To take a look at the original full data, take a look at `NerdyDataFull.csv`. The remaining files address measures and data from that set.<br>

## More About the Packages

1. The `narian` package has alot of functionality not covered here. Take a look at the following for more help

    * [Getting Started with naniar](https://naniar.njtierney.com/articles/getting-started-w-naniar.html#introduction){target="_blank"}

    * [Gallery of Missing Data Visualisations](https://naniar.njtierney.com/articles/naniar-visualisation.html){target="_blank"}

2. The `mice` package is confusing at times and can hurt your head, but its by far one of the most versatile and powerful packages to impute data. So if you find yourself with missing data, it may be beneficial to dive in headfirst. To get acquainted, a couple vignettes can be found using the link below 

    * [miceVignettes](https://www.gerkovink.com/miceVignettes/){target="_blank"}
