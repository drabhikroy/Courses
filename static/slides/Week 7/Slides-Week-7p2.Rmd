---
title: "Sampling and Sampling Distributions Part II"
subtitle: "EDP 613"
author: "Week 7"
output: 
 xaringan::moon_reader:
   css: xaringan-themer.css
   nature:
     ratio: 16:9
     highlightStyle: github
     highlightLines: true
     countIncrementalSlides: false
     navigation:
         scroll: false
---

```{r setup, include=FALSE, purl=FALSE}
knitr::opts_chunk$set(echo = FALSE)

library(tidyverse)
library(knitr)
library(kableExtra)
library(fontawesome)
library(here)
library(DT)
library(scales)
library(latex2exp)
library(showtext)
font_add_google("Roboto Condensed", "roboto")
showtext_auto()
```

```{r echo = FALSE, purl=FALSE}
xaringanthemer::style_duo(
  primary_color = "#212121",
  secondary_color = "#bff4ee",
  table_row_border_color = "#212121",
  table_row_even_background_color = "#212121",
  footnote_font_size = "0.6em",
  header_font_google = xaringanthemer::google_font("Roboto Condensed", "700"),
  text_font_google   = xaringanthemer::google_font("Roboto Condensed", "400")
)
xaringanExtra::use_xaringan_extra(c("tile_view", "animate_css", "tachyons"))

xaringanExtra::use_logo(
  image_url = here::here("static", "img", "course_hex_alpha.png"),
  link_url = "https://edp613.asocialdatascientist.com",
  position = xaringanExtra::css_position(top = "1em", right = "1em")
)
```

```{r echo = FALSE}
shading_geq <- function(x, lower_bound) {
  y = dnorm(x, mean = m, sd = stdev)
  y[x < lower_bound] <- NA
  return(y)
}

shading_leq <- function(x, upper_bound) {
  y = dnorm(x, mean = m, sd = stdev)
  y[x > upper_bound] <- NA
  return(y)
}

shading_beq <- function(x, lower_bound, upper_bound) {
  y = dnorm(x, mean = m, sd = stdev)
  y[x < lower_bound | x > upper_bound] <- NA
  return(y)
}

shading_neq <- function(x, lower_bound, upper_bound) {
  y = dnorm(x, mean = m, sd = stdev)
  y[x > lower_bound | x < upper_bound] <- NA
  return(y)
}
```

# A Note About The Slides

Currently the equations do not show up properly in Firefox. Other browsers such as Chrome and Safari do work. 

---

# A Note About Probability

We're going to touch on this now but come back to more of it later in the term when talking about Bayesian Statistics.

---

# For Now

--

- An **event** $E$ is a set of outcomes of an experiment.

--

- The **probability** $P$ of an event describes how likely it will occur.

--

-  A **sample space** contains all possible outcomes. 

--

-  A **probability distribution** gives a probability for each value in a sample space. 


---

# Example

> What is the sample space and probability distribution created by tossing a fair quarter?

--

>   >- Sample space: {*Heads*, *Tails*}

--

>   >- Probability distribution: $\left\{\dfrac{1}{2}, \dfrac{1}{2}\right\}$

---

## Notions

--

>- The probability of an event is ALWAYS between 0 and 1. 

--

>- Assuming all outcomes are likely, the probability $P$ of an event $E$ can be found
$$P(E) = \frac{\mathrm{Number\,\,of\,\,times\,\,an\,\,event\,\,will\,\,happen}}{\mathrm{Total\,\,number\,\,of\,\,events}}$$
---

## Example

>- Assume that a standard fair six sided die is rolled. Find the 

--

>>- (a) sample space and then 

--

>>- (b) the probability that someone will roll a 2

<br>
--
<br>

>>- (a) The sample space of event $E=$ six sided dice is rolled is $P(E)=\{1,2,3,4,5,6\}$

--

>   >- (b) The probability that someone will roll a 2 is $P(2)$ which can be found by $$P(2)=\dfrac{1}{6}$$

---

## On Your Own (More of a Challenge!)

>- Assume that a standard fair six sided die is rolled. Find the (a) the probability that someone will roll a 7 and (b) the probability that someone will roll less than a 3

--

>>- (a) The probability that someone will roll a 7 is $P(7)$ which can be found by $$P(7)=\dfrac{0}{6}$$ since the sample space is $P(E)=\{1,2,3,4,5,6\}$

---

>>- (b) The probability that someone will roll less than a 3 is $P(<3)$ which can be found by 
\begin{aligned}
P(<3)&=P(1)+P(2)\\\\
&=\dfrac{1}{6} + \dfrac{1}{6}\\\\
&= \dfrac{2}{6} \\\\
&= \dfrac{1}{3}
\end{aligned}

---

### Rule: *Always Reduce Fractions*

--

- But why?

--

- $\dfrac{2}{6} = \dfrac{1}{3}$ but what do you lose by reducing?

--

- The sample size information which seems sort of important!

---

### New Rule: *Don't Reduce Fractions Unless it Makes Sense!*

---

## Sampling Distributions

- If several samples are drawn from a population, they are likely to have different values for for the mean $\overline{Y}$

--

- The probability distribution of those means (aka all of the $\overline{Y}\mathrm{s}$) is called the **sampling distribution**

---

## Sampling Distributions: Words and Notation - The Mean

--

 The mean is calculated the ***exact same way*** as always but

--

- is called the *mean of the sampling distribution*

--

  - has special variables:

--

  - represented by $\mu_{\overline{Y}}$

--

  - sample size is specifically for probabilities and represented by $M$
        
--

  - given by the formula:
$$\mu_{\overline{Y}}=\dfrac{\overline{Y}}{M}$$
  
---

## Sampling Distributions: Words and Notation - The Standard Deviation

--

- The standard deviation is calculated the ***exact same way*** as always but

--

   - is called the *standard error of the mean*

--

  - has special variables:

     - represented by $\sigma_{\overline{Y}}$

     - sample size is specifically for probabilities and represented by $N$

--

  - given by the formula: $$\sigma_{\overline{Y}}=\dfrac{\sigma}{\sqrt{N}}$$
  
---

## Central Limit Theorem (CLT)

>- *Officially*: If $\overline{Y}$ is the mean of a large SRS ( $N>30$ ) from a population with mean $\mu$ and standard deviation $\sigma$, as $M$ increases, the distribution becomes normal

--

>- *Better*: As you take more samples, especially big ones, your graph of the sample means will look more like a normal distribution

--

>- Implications

--

>>- If you add up the means from all of your samples and find the average, that number will be your *actual population mean*.<br><br>
    
--

>>- If you add up the standard deviations from all of your samples and find the average, that number will be your *actual population standard deviation*.<br><br>
    
--

>>- Helps you predict characteristics of a population

---

## Procedure for Calculating the CLT

1. Be sure $N>30$

--

2. Find $\mu_{\overline{Y}}$ and  $\sigma_{\overline{Y}}$

--

3. Sketch a normal curve and shade in the area to be found

--

4. Find the area using The Standard Normal Table (Appendix B)

---

## Example

According to the Nielsen Company, the mean number of TV sets in a U.S. household in 2008 was 2.83. Assume the standard deviation is 1.2. A sample of 85 households is drawn. What is the probability that the sample mean number of TV sets is between 2.5 and 3?

--

1. $85>30$ so this is probably normal

--

2. We have $$\mu_{\overline{Y}}=2.83$$ with 

\begin{aligned}
\sigma_{\overline{Y}}&=\dfrac{1.2}{\sqrt{85}}\\\\
&\approx 0.130158
\end{aligned}


---

<ol start="3">
<li><br>

```{r echo = FALSE, eval= TRUE, warning = FALSE, out.width = '40%'}

minimum = 2.5
maximum = 3
m = 2.83
n = 85
stdev = 1.2

f <- function(x) n*x*pnorm(x)^(n-1)*dnorm(x) 
num <- integrate(f,-Inf,Inf) 
ci <- 2 * num$value

lb <- m - ci
ub <- m + ci

ll <- round(lb)
ul <- round(ub)

ggplot(data.frame(x = c(lb, ub)), aes(x = x)) + 
  stat_function(fun = dnorm, 
                args = list(mean = m, sd = stdev),
                color = "#ffffff",
                size = 3) + 
  stat_function(fun = shading_beq, 
                args = list(lower_bound = minimum,
                            upper_bound = maximum), 
                geom = "area", 
                fill = "#5bc0de") +
  scale_x_continuous(breaks = c(ll, m, ul),
                     limits = c(2, 3.5)) +
  geom_vline(xintercept = m, 
             color = "#ffffff",
             lty = "dashed",
             size = 2) +
  theme_void(base_family = "roboto") +
  theme(
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(),
        panel.background = element_rect(fill = "#212121",color = NA),
        plot.background = element_rect(fill = "#212121",color = NA),
        axis.text.x = element_text(size = 40,
                                   color = "#ffffff",
                                   margin = margin(t = 30))
        )
```

</li>
</ol>

--

<ol start="4">
<li>We have `z`-scores<br>

.pull-left[
\begin{aligned}
z&=\dfrac{3-2.83}{0.130158}\\\\
&\approx 1.31
\end{aligned}
]

.pull-right[
\begin{aligned}
z&=\dfrac{2.5-2.83}{0.130158}\\\\
&\approx -2.54
\end{aligned}
]

</li>
</ol>

---

```{r echo = FALSE, eval= TRUE, warning = FALSE, out.width = '40%'}

minimum = 2.5
maximum = 3
m = 2.83
n = 85
stdev = 1.2

f <- function(x) n*x*pnorm(x)^(n-1)*dnorm(x) 
num <- integrate(f,-Inf,Inf) 
ci <- 2 * num$value

lb <- m - ci
ub <- m + ci

ll <- round(lb)
ul <- round(ub)

ggplot(data.frame(x = c(lb, ub)), aes(x = x)) + 
  stat_function(fun = dnorm, 
                args = list(mean = m, sd = stdev),
                color = "#ffffff",
                size = 3) + 
  stat_function(fun = shading_beq, 
                args = list(lower_bound = minimum,
                            upper_bound = maximum), 
                geom = "area", 
                fill = "#5bc0de") +
  scale_x_continuous(breaks = c(minimum, m, maximum),
                     limits = c(2, 3.5),
                     labels = c(as.character(minimum), 
                                "",
                                as.character(maximum))) +
  geom_vline(xintercept = m, 
             color = "#ffffff",
             lty = "dashed",
             size = 2) +
  theme_void(base_family = "roboto") +
  theme(
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(),
        panel.background = element_rect(fill = "#212121",color = NA),
        plot.background = element_rect(fill = "#212121",color = NA),
        axis.text.x = element_text(size = 40,
                                   color = "#ffffff")
        )
```

--

- The Standard Normal Table tells us that this is $0.8994$

--

- So there was about a 90% chance that a random household had between 2.5 and 3 TVs in 2008.

---

## Example

It is estimated that the mean number of TV sets in a U.S. household in 2020 is 2.00. Assume the standard deviation is 0.8. A sample of 180 households is drawn. What is the probability that the sample mean number of TV sets is still between 2.5 and 3?

--

1. $180>30$ so this is probably normal 

--

2. We have $$\mu_{\overline{Y}}=2.00$$ with 

\begin{aligned}
\sigma_{\overline{Y}}&=\dfrac{0.8}{\sqrt{180}}\\\\
&\approx 0.059628
\end{aligned}

---

<ol start="3">
    <li> <br>
```{r echo = FALSE, eval= TRUE, warning = FALSE, out.width = '40%'}

minimum = 2.5
maximum = 3
m = 2.00
n = 180
stdev = 0.8

f <- function(x) n*x*pnorm(x)^(n-1)*dnorm(x) 
num <- integrate(f,-Inf,Inf) 
ci <- 2 * num$value

lb <- m - ci
ub <- m + ci

ll <- round(lb)
ul <- round(ub)

ggplot(data.frame(x = c(lb, ub)), aes(x = x)) + 
  stat_function(fun = dnorm, 
                args = list(mean = m, sd = stdev),
                color = "#ffffff",
                size = 3) + 
  stat_function(fun = shading_beq, 
                args = list(lower_bound = minimum,
                            upper_bound = maximum), 
                geom = "area", 
                fill = "#5bc0de") +
  scale_x_continuous(breaks = c(ll, m, ul),
                     limits = c(2, 3.5)) +
  geom_vline(xintercept = m, 
             color = "#ffffff",
             lty = "dashed",
             size = 2) +
  theme_void(base_family = "roboto") +
  theme(
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(),
        panel.background = element_rect(fill = "#212121",color = NA),
        plot.background = element_rect(fill = "#212121",color = NA),
        axis.text.x = element_text(size = 40,
                                   color = "#ffffff",
                                   margin = margin(t = 30))
        )
```

</li>
<br>
<li>We have `z`-scores<br> 

.pull-left[
\begin{aligned}
z&=\dfrac{3-2.00}{0.059628}\\\\
&\approx 16.77
\end{aligned}
]

.pull-right[
\begin{aligned}
z&=\dfrac{2.5-2.00}{0.059628}\\\\
&\approx 8.38
\end{aligned}
]
</li>
</ol>

---

```{r echo = FALSE, eval= TRUE, warning = FALSE, out.width = '40%'}

minimum = 2.5
maximum = 3
m = 2.00
n = 180
stdev = 0.8

f <- function(x) n*x*pnorm(x)^(n-1)*dnorm(x) 
num <- integrate(f,-Inf,Inf) 
ci <- 2 * num$value

lb <- m - ci
ub <- m + ci

ll <- round(lb)
ul <- round(ub)

ggplot(data.frame(x = c(lb, ub)), aes(x = x)) + 
  stat_function(fun = dnorm, 
                args = list(mean = m, sd = stdev),
                color = "#ffffff",
                size = 3) + 
  stat_function(fun = shading_beq, 
                args = list(lower_bound = minimum,
                            upper_bound = maximum), 
                geom = "area", 
                fill = "#5bc0de") +
  scale_x_continuous(breaks = c(minimum, m, maximum),
                     limits = c(2, 3.5),
                     labels = c(as.character(minimum), 
                                "",
                                as.character(maximum))) +
  geom_vline(xintercept = m, 
             color = "#ffffff",
             lty = "dashed",
             size = 2) +
  theme_void(base_family = "roboto") +
  theme(
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(),
        panel.background = element_rect(fill = "#212121",color = NA),
        plot.background = element_rect(fill = "#212121",color = NA),
        axis.text.x = element_text(size = 40,
                                   color = "#ffffff")
        )
```

--

- The Standard Normal Table tells us that this is essentially $0$

--

- So there is nearly a 0% chance that a random household has between 2.5 and 3 TVs in 2020.

---

## That's it. Let's take a break before working in R.
